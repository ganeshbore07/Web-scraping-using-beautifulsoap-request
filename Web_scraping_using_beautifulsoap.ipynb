{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/SsS67HCK1aNCqPay4yKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshbore07/ganeshbore07/blob/main/Web_scraping_using_beautifulsoap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "bJt_Wg-Spi4p"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://books.toscrape.com/'\n",
        "response = requests.get(URL)"
      ],
      "metadata": {
        "id": "K2Jxp9cxp43X"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_contents = response.text\n",
        "#Creating a file and loading the page contents in it.\n",
        "with open('Bookswebpage.html','w') as f:\n",
        "    f.write(page_contents)"
      ],
      "metadata": {
        "id": "YxOijxutqEq5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "doc = BeautifulSoup(page_contents,'html.parser')"
      ],
      "metadata": {
        "id": "kuAy443UqNLK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_book_titles(doc):\n",
        "    Book_title_tags = doc.find_all('h3')\n",
        "    Book_titles = []\n",
        "    for tags in Book_title_tags:\n",
        "        Book_titles.append(tags.text)\n",
        "    return Book_titles"
      ],
      "metadata": {
        "id": "B1Lml1FeqYd-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_book_titles(doc):\n",
        "    Book_title_tags = doc.find_all('h3')\n",
        "    Book_titles = []\n",
        "    for tags in Book_title_tags:\n",
        "        Book_titles.append(tags.text)\n",
        "    return Book_titles"
      ],
      "metadata": {
        "id": "c4lvf9-0qhfc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_book_price(doc):\n",
        "    Book_price_tags = doc.find_all('p', class_ = 'price_color')\n",
        "    Book_price = []\n",
        "    for tags in Book_price_tags:\n",
        "        Book_price.append(tags.text.replace('Ã‚',''))\n",
        "    return Book_price"
      ],
      "metadata": {
        "id": "exN1MNh-qn6p"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_availability(doc):\n",
        "    Book_stock_tags = doc.find_all('p', class_ = 'instock availability')\n",
        "    Book_stock = []\n",
        "    for tags in Book_stock_tags:\n",
        "        Book_stock.append(tags.text.strip())\n",
        "    return Book_stock"
      ],
      "metadata": {
        "id": "h7S90tTUq5Or"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_book_url(Book_title_tags):\n",
        "    Book_url = []\n",
        "    for article in Book_title_tags:\n",
        "        for link in article.find_all('a', href = True):\n",
        "            url = link['href']\n",
        "            links = 'https://books.toscrape.com/' + url\n",
        "            if links not in Book_url:\n",
        "                Book_url.append(links)\n",
        "    return Book_url"
      ],
      "metadata": {
        "id": "Pja5Bne1rJn7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc(url):\n",
        "    response = requests.get(url)\n",
        "    doc = BeautifulSoup(response.text,'html.parser')\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(response))\n",
        "    return doc"
      ],
      "metadata": {
        "id": "GuRtv4YitfcG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_multiple_pages(n):\n",
        "    URL = 'https://books.toscrape.com/catalogue/page-'\n",
        "    titles,prices,stocks_availability,urls = [],[],[],[]\n",
        "\n",
        "    for page in range(1,n+1):\n",
        "        doc = get_doc(URL + str(page)+ '.html')\n",
        "        titles.extend(get_book_titles(doc))\n",
        "        prices.extend(get_book_price(doc))\n",
        "        stocks_availability.extend(get_stock_availability(doc))\n",
        "        urls.extend(get_book_url(doc.find_all('h3')))\n",
        "\n",
        "    book_dict1 = {\n",
        "                'TITLE':titles,\n",
        "                'PRICE':prices,\n",
        "                'STOCK AVAILABILTY':stocks_availability,\n",
        "                'URL':urls}\n",
        "    return pd.DataFrame(book_dict1)"
      ],
      "metadata": {
        "id": "efrKNcmHrLiD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_multiple_pages(5).to_csv('SCB.csv',index = None)"
      ],
      "metadata": {
        "id": "C9T1000btMsL"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}